{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bea22b2-7a2b-4cb4-b355-5c97b22ec499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bet/lerobot/src/lerobot/policies/vqbet/vqbet_utils.py:1385: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "WARNING:root:Device 'None' is not available. Switching to 'cuda'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 26.00M\n",
      "self._offset_loss_multiplier: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe8ec03c68743be901a939e542ccf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: -0.000\n",
      "step: 100 loss: -0.000\n",
      "step: 200 loss: -0.000\n",
      "step: 300 loss: -0.000\n",
      "step: 400 loss: -0.000\n",
      "step: 500 loss: -0.000\n",
      "step: 600 loss: -0.000\n",
      "step: 700 loss: -0.000\n",
      "step: 800 loss: -0.000\n",
      "step: 900 loss: -0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-means clustering: 100%|███████████████████████████████████████| 100/100 [00:12<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1000 loss: 205.325\n",
      "step: 1100 loss: 8.672\n",
      "step: 1200 loss: 8.614\n",
      "step: 1300 loss: 10.484\n",
      "step: 1400 loss: 9.117\n",
      "step: 1500 loss: 8.791\n",
      "step: 1600 loss: 8.839\n",
      "step: 1700 loss: 9.316\n",
      "step: 1800 loss: 8.467\n",
      "step: 1900 loss: 9.395\n",
      "step: 2000 loss: 8.994\n",
      "step: 2100 loss: 7.850\n",
      "step: 2200 loss: 7.256\n",
      "step: 2300 loss: 7.077\n",
      "step: 2400 loss: 6.213\n",
      "step: 2500 loss: 5.487\n",
      "step: 2600 loss: 5.856\n",
      "step: 2700 loss: 5.712\n",
      "step: 2800 loss: 5.334\n",
      "step: 2900 loss: 6.595\n",
      "step: 3000 loss: 5.199\n",
      "step: 3100 loss: 5.516\n",
      "step: 3200 loss: 5.704\n",
      "step: 3300 loss: 5.428\n",
      "step: 3400 loss: 5.347\n",
      "step: 3500 loss: 4.849\n",
      "step: 3600 loss: 4.391\n",
      "step: 3700 loss: 4.637\n",
      "step: 3800 loss: 4.126\n",
      "step: 3900 loss: 4.970\n",
      "step: 4000 loss: 4.725\n",
      "step: 4100 loss: 4.274\n",
      "step: 4200 loss: 4.028\n",
      "step: 4300 loss: 3.412\n",
      "step: 4400 loss: 3.477\n",
      "step: 4500 loss: 3.899\n",
      "step: 4600 loss: 4.305\n",
      "step: 4700 loss: 4.379\n",
      "step: 4800 loss: 4.677\n",
      "step: 4900 loss: 3.780\n",
      "step: 5000 loss: 4.410\n",
      "step: 5100 loss: 3.554\n",
      "step: 5200 loss: 3.716\n",
      "step: 5300 loss: 3.807\n",
      "step: 5400 loss: 3.312\n",
      "step: 5500 loss: 3.498\n",
      "step: 5600 loss: 3.406\n",
      "step: 5700 loss: 4.538\n",
      "step: 5800 loss: 3.776\n",
      "step: 5900 loss: 3.962\n",
      "step: 6000 loss: 3.510\n",
      "step: 6100 loss: 3.782\n",
      "step: 6200 loss: 3.691\n",
      "step: 6300 loss: 3.718\n",
      "step: 6400 loss: 3.311\n",
      "step: 6500 loss: 3.260\n",
      "step: 6600 loss: 3.386\n",
      "step: 6700 loss: 3.716\n",
      "step: 6800 loss: 2.546\n",
      "step: 6900 loss: 3.039\n",
      "step: 7000 loss: 3.138\n",
      "step: 7100 loss: 2.910\n",
      "step: 7200 loss: 3.316\n",
      "step: 7300 loss: 3.123\n",
      "step: 7400 loss: 3.045\n",
      "step: 7500 loss: 2.932\n",
      "step: 7600 loss: 3.578\n",
      "step: 7700 loss: 3.260\n",
      "step: 7800 loss: 3.408\n",
      "step: 7900 loss: 3.055\n",
      "step: 8000 loss: 3.540\n",
      "step: 8100 loss: 4.262\n",
      "step: 8200 loss: 2.871\n",
      "step: 8300 loss: 3.181\n",
      "step: 8400 loss: 2.889\n",
      "step: 8500 loss: 3.404\n",
      "step: 8600 loss: 2.869\n",
      "step: 8700 loss: 3.090\n",
      "step: 8800 loss: 2.849\n",
      "step: 8900 loss: 2.717\n",
      "step: 9000 loss: 3.003\n",
      "step: 9100 loss: 2.817\n",
      "step: 9200 loss: 3.574\n",
      "step: 9300 loss: 3.359\n",
      "step: 9400 loss: 3.009\n",
      "step: 9500 loss: 3.330\n",
      "step: 9600 loss: 2.971\n",
      "step: 9700 loss: 3.060\n",
      "step: 9800 loss: 2.844\n",
      "step: 9900 loss: 2.438\n",
      "step: 10000 loss: 2.616\n",
      "step: 10100 loss: 2.830\n",
      "step: 10200 loss: 3.392\n",
      "step: 10300 loss: 2.506\n",
      "step: 10400 loss: 2.887\n",
      "step: 10500 loss: 2.903\n",
      "step: 10600 loss: 2.870\n",
      "step: 10700 loss: 2.815\n",
      "step: 10800 loss: 2.889\n",
      "step: 10900 loss: 2.819\n",
      "step: 11000 loss: 2.522\n",
      "step: 11100 loss: 2.617\n",
      "step: 11200 loss: 2.627\n",
      "step: 11300 loss: 2.583\n",
      "step: 11400 loss: 2.569\n",
      "step: 11500 loss: 2.791\n",
      "step: 11600 loss: 2.547\n",
      "step: 11700 loss: 2.558\n",
      "step: 11800 loss: 2.632\n",
      "step: 11900 loss: 2.603\n",
      "step: 12000 loss: 2.532\n",
      "step: 12100 loss: 3.014\n",
      "step: 12200 loss: 2.775\n",
      "step: 12300 loss: 2.542\n",
      "step: 12400 loss: 2.646\n",
      "step: 12500 loss: 2.489\n",
      "step: 12600 loss: 2.598\n",
      "step: 12700 loss: 2.644\n",
      "step: 12800 loss: 2.686\n",
      "step: 12900 loss: 2.845\n",
      "step: 13000 loss: 2.824\n",
      "step: 13100 loss: 2.749\n",
      "step: 13200 loss: 2.440\n",
      "step: 13300 loss: 2.722\n",
      "step: 13400 loss: 2.345\n",
      "step: 13500 loss: 2.456\n",
      "step: 13600 loss: 2.447\n",
      "step: 13700 loss: 2.549\n",
      "step: 13800 loss: 2.269\n",
      "step: 13900 loss: 2.427\n",
      "step: 14000 loss: 2.327\n",
      "step: 14100 loss: 2.681\n",
      "step: 14200 loss: 2.477\n",
      "step: 14300 loss: 2.501\n",
      "step: 14400 loss: 2.215\n",
      "step: 14500 loss: 2.527\n",
      "step: 14600 loss: 2.196\n",
      "step: 14700 loss: 2.927\n",
      "step: 14800 loss: 2.667\n",
      "step: 14900 loss: 2.536\n",
      "step: 15000 loss: 2.465\n",
      "step: 15100 loss: 2.654\n",
      "step: 15200 loss: 2.481\n",
      "step: 15300 loss: 2.253\n",
      "step: 15400 loss: 2.381\n",
      "step: 15500 loss: 2.720\n",
      "step: 15600 loss: 2.520\n",
      "step: 15700 loss: 2.393\n",
      "step: 15800 loss: 2.166\n",
      "step: 15900 loss: 1.963\n",
      "step: 16000 loss: 2.252\n",
      "step: 16100 loss: 2.191\n",
      "step: 16200 loss: 2.148\n",
      "step: 16300 loss: 2.079\n",
      "step: 16400 loss: 2.217\n",
      "step: 16500 loss: 2.091\n",
      "step: 16600 loss: 1.994\n",
      "step: 16700 loss: 2.770\n",
      "step: 16800 loss: 2.259\n",
      "step: 16900 loss: 2.048\n",
      "step: 17000 loss: 2.054\n",
      "step: 17100 loss: 2.220\n",
      "step: 17200 loss: 2.045\n",
      "step: 17300 loss: 1.985\n",
      "step: 17400 loss: 2.427\n",
      "step: 17500 loss: 2.506\n",
      "step: 17600 loss: 2.199\n",
      "step: 17700 loss: 1.875\n",
      "step: 17800 loss: 2.104\n",
      "step: 17900 loss: 2.187\n",
      "step: 18000 loss: 1.714\n",
      "step: 18100 loss: 1.932\n",
      "step: 18200 loss: 1.968\n",
      "step: 18300 loss: 2.111\n",
      "step: 18400 loss: 2.031\n",
      "step: 18500 loss: 2.104\n",
      "step: 18600 loss: 1.938\n",
      "step: 18700 loss: 2.006\n",
      "step: 18800 loss: 2.197\n",
      "step: 18900 loss: 2.044\n",
      "step: 19000 loss: 2.330\n",
      "step: 19100 loss: 2.325\n",
      "step: 19200 loss: 2.348\n",
      "step: 19300 loss: 2.104\n",
      "step: 19400 loss: 1.823\n",
      "step: 19500 loss: 2.172\n",
      "step: 19600 loss: 2.148\n",
      "step: 19700 loss: 1.724\n",
      "step: 19800 loss: 2.154\n",
      "step: 19900 loss: 2.102\n",
      "step: 20000 loss: 1.979\n",
      "step: 20100 loss: 1.927\n",
      "step: 20200 loss: 1.801\n",
      "step: 20300 loss: 1.829\n",
      "step: 20400 loss: 1.866\n",
      "step: 20500 loss: 1.874\n",
      "step: 20600 loss: 1.989\n",
      "step: 20700 loss: 2.015\n",
      "step: 20800 loss: 2.155\n",
      "step: 20900 loss: 1.641\n",
      "step: 21000 loss: 1.823\n",
      "step: 21100 loss: 1.719\n",
      "step: 21200 loss: 1.857\n",
      "step: 21300 loss: 1.620\n",
      "step: 21400 loss: 1.718\n",
      "step: 21500 loss: 1.916\n",
      "step: 21600 loss: 1.567\n",
      "step: 21700 loss: 2.130\n",
      "step: 21800 loss: 2.121\n",
      "step: 21900 loss: 1.573\n",
      "step: 22000 loss: 1.478\n",
      "step: 22100 loss: 1.843\n",
      "step: 22200 loss: 1.474\n",
      "step: 22300 loss: 2.281\n",
      "step: 22400 loss: 1.655\n",
      "step: 22500 loss: 1.645\n",
      "step: 22600 loss: 2.026\n",
      "step: 22700 loss: 1.771\n",
      "step: 22800 loss: 1.637\n",
      "step: 22900 loss: 1.548\n",
      "step: 23000 loss: 1.763\n",
      "step: 23100 loss: 1.625\n",
      "step: 23200 loss: 1.664\n",
      "step: 23300 loss: 1.662\n",
      "step: 23400 loss: 1.923\n",
      "step: 23500 loss: 1.722\n",
      "step: 23600 loss: 1.693\n",
      "step: 23700 loss: 1.758\n",
      "step: 23800 loss: 1.743\n",
      "step: 23900 loss: 1.779\n",
      "step: 24000 loss: 1.654\n",
      "step: 24100 loss: 1.425\n",
      "step: 24200 loss: 1.912\n",
      "step: 24300 loss: 1.484\n",
      "step: 24400 loss: 1.633\n",
      "step: 24500 loss: 1.552\n",
      "step: 24600 loss: 1.899\n",
      "step: 24700 loss: 1.627\n",
      "step: 24800 loss: 1.812\n",
      "step: 24900 loss: 1.878\n",
      "step: 25000 loss: 1.646\n",
      "step: 25100 loss: 1.537\n",
      "step: 25200 loss: 1.329\n",
      "step: 25300 loss: 1.449\n",
      "step: 25400 loss: 1.553\n",
      "step: 25500 loss: 1.725\n",
      "step: 25600 loss: 1.704\n",
      "step: 25700 loss: 1.514\n",
      "step: 25800 loss: 1.523\n",
      "step: 25900 loss: 1.642\n",
      "step: 26000 loss: 1.411\n",
      "step: 26100 loss: 1.581\n",
      "step: 26200 loss: 1.647\n",
      "step: 26300 loss: 1.644\n",
      "step: 26400 loss: 1.524\n",
      "step: 26500 loss: 1.474\n",
      "step: 26600 loss: 1.481\n",
      "step: 26700 loss: 1.502\n",
      "step: 26800 loss: 1.340\n",
      "step: 26900 loss: 1.501\n",
      "step: 27000 loss: 1.505\n",
      "step: 27100 loss: 1.720\n",
      "step: 27200 loss: 1.308\n",
      "step: 27300 loss: 1.445\n",
      "step: 27400 loss: 1.734\n",
      "step: 27500 loss: 1.617\n",
      "step: 27600 loss: 1.646\n",
      "step: 27700 loss: 1.577\n",
      "step: 27800 loss: 1.397\n",
      "step: 27900 loss: 1.737\n",
      "step: 28000 loss: 1.475\n",
      "step: 28100 loss: 1.672\n",
      "step: 28200 loss: 1.482\n",
      "step: 28300 loss: 1.876\n",
      "step: 28400 loss: 1.248\n",
      "step: 28500 loss: 1.499\n",
      "step: 28600 loss: 1.513\n",
      "step: 28700 loss: 1.470\n",
      "step: 28800 loss: 1.490\n",
      "step: 28900 loss: 1.493\n",
      "step: 29000 loss: 1.417\n",
      "step: 29100 loss: 1.368\n",
      "step: 29200 loss: 1.345\n",
      "step: 29300 loss: 1.484\n",
      "step: 29400 loss: 1.501\n",
      "step: 29500 loss: 1.417\n",
      "step: 29600 loss: 1.174\n",
      "step: 29700 loss: 1.447\n",
      "step: 29800 loss: 1.360\n",
      "step: 29900 loss: 1.566\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from lerobot.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.policies.bet.configuration_bet import BeTConfig\n",
    "from lerobot.policies.bet.modeling_bet import BeTPolicy\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Create a directory to store the training checkpoint.\n",
    "output_directory = Path(\"outputs/train/test_bet_30k_off1000\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "training_steps = 30000\n",
    "losses = []\n",
    "classification_losses = []\n",
    "offset_losses = []\n",
    "log_freq = 100\n",
    "\n",
    "# When starting from scratch (i.e. not from a pretrained policy), we need to specify 2 things before\n",
    "# creating the policy:\n",
    "#   - input/output shapes: to properly size the policy\n",
    "#   - dataset stats: for normalization and denormalization of input/outputs\n",
    "dataset_metadata = LeRobotDatasetMetadata(\"lerobot/pusht\")\n",
    "features = dataset_to_policy_features(dataset_metadata.features)\n",
    "output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "input_features = {key: ft for key, ft in features.items() if key not in output_features}\n",
    "\n",
    "# Policies are initialized with a configuration class. For this example,\n",
    "# we'll just use the defaults and so no arguments other than input/output features need to be passed.\n",
    "cfg = BeTConfig(input_features=input_features, output_features=output_features, offset_loss_multiplier=1000)\n",
    "\n",
    "# We can now instantiate our policy with this config and the dataset stats.\n",
    "policy = BeTPolicy(cfg, dataset_stats=dataset_metadata.stats)\n",
    "# load from pretrained\n",
    "# pretrained_policy_path = Path(\"outputs/train/test_bet\")\n",
    "# policy = BeTPolicy.from_pretrained(pretrained_policy_path)\n",
    "\n",
    "policy.train()\n",
    "policy.to(device)\n",
    "\n",
    "# Another policy-dataset interaction is with the delta_timestamps. Each policy expects a given number frames\n",
    "# which can differ for inputs, outputs and rewards (if there are some).\n",
    "delta_timestamps = {\n",
    "    \"observation.image\": [i / dataset_metadata.fps for i in cfg.observation_delta_indices],\n",
    "    \"observation.state\": [i / dataset_metadata.fps for i in cfg.observation_delta_indices],\n",
    "    \"action\": [i / dataset_metadata.fps for i in cfg.action_delta_indices],\n",
    "}\n",
    "\n",
    "# standard configuration for new BeT, it is equivalent to this:\n",
    "# delta_timestamps = {\n",
    "#     # Load the previous image and state at -0.1 seconds before current frame,\n",
    "#     # then load current image and state corresponding to 0.0 second.\n",
    "#     \"observation.image\": [-0.4, -0.3, -0.2, -0.1, 0.0],\n",
    "#     \"observation.state\": [-0.4, -0.3, -0.2, -0.1, 0.0],\n",
    "#     # Load the previous action (-0.1), the next action to be executed (0.0), and one more into future (0.1) All these actions will be\n",
    "#     # used to supervise the policy; for BeT I got n_action_pred_token: int = 3\n",
    "#     \"action\": [-0.1, 0.0, 0.1],\n",
    "# }\n",
    "\n",
    "# We can then instantiate the dataset with these delta_timestamps configuration.\n",
    "dataset = LeRobotDataset(\"lerobot/pusht\", delta_timestamps=delta_timestamps)\n",
    "\n",
    "# Then we create our optimizer and dataloader for offline training.\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# Run training loop.\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    for batch in dataloader:\n",
    "        batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        loss, loss_dict = policy.forward(batch)\n",
    "        cl_loss, offset_loss = loss_dict.pop('classification_loss'), loss_dict.pop('offset_loss')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % log_freq == 0:\n",
    "            print(f\"step: {step} loss: {loss.item():.3f}\")\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        classification_losses.append(cl_loss)\n",
    "        offset_losses.append(offset_loss)\n",
    "        step += 1\n",
    "        if step >= training_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "policy.save_pretrained(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6544f028-e8b1-43b0-a82d-97a33452d7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/lerobot/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'observation.image': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 96, 96)), 'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(2,))}\n",
      "Dict('agent_pos': Box(0.0, 512.0, (2,), float64), 'pixels': Box(0, 255, (96, 96, 3), uint8))\n",
      "{'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(2,))}\n",
      "Box(0.0, 512.0, (2,), float32)\n",
      "numpy_action [169.76578 411.27423]\n",
      "step=0 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [193.74083 408.32437]\n",
      "step=1 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [193.11534 431.39883]\n",
      "step=2 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [234.46233 423.3732 ]\n",
      "step=3 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [204.08383 419.43323]\n",
      "step=4 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [252.39441 434.70892]\n",
      "step=5 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [256.12677 430.1405 ]\n",
      "step=6 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [315.72968 440.87143]\n",
      "step=7 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [334.06458 438.3397 ]\n",
      "step=8 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [346.80634 431.1475 ]\n",
      "step=9 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [410.78934 427.0231 ]\n",
      "step=10 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [384.10324 421.5982 ]\n",
      "step=11 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [411.60602 413.18945]\n",
      "step=12 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [422.5896  409.22525]\n",
      "step=13 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [425.24045 403.5698 ]\n",
      "step=14 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [435.09076 374.50354]\n",
      "step=15 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [444.9818  364.37463]\n",
      "step=16 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [448.17923 348.14175]\n",
      "step=17 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [451.1921  337.14038]\n",
      "step=18 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [451.8811  325.76108]\n",
      "step=19 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [453.6379  321.51898]\n",
      "step=20 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [455.9642 314.2601]\n",
      "step=21 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [459.49478 306.42215]\n",
      "step=22 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [395.3503  294.92252]\n",
      "step=23 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [434.94168 288.74384]\n",
      "step=24 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [428.86923 286.04495]\n",
      "step=25 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [420.66058 286.59384]\n",
      "step=26 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [417.98727 283.24692]\n",
      "step=27 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [422.48407 280.2492 ]\n",
      "step=28 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [415.66922 281.77722]\n",
      "step=29 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [417.39328 279.26785]\n",
      "step=30 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [407.83115 278.7022 ]\n",
      "step=31 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [402.66327 271.43896]\n",
      "step=32 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [397.7092 266.5355]\n",
      "step=33 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [388.15277 257.47192]\n",
      "step=34 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [393.3917  251.35837]\n",
      "step=35 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [387.12515 244.5672 ]\n",
      "step=36 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [382.74036 230.84412]\n",
      "step=37 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [381.1194  232.32484]\n",
      "step=38 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [376.92383 222.7952 ]\n",
      "step=39 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [378.72052 212.57063]\n",
      "step=40 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [370.00186 213.17563]\n",
      "step=41 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [368.18372 196.17664]\n",
      "step=42 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [369.90823 194.59479]\n",
      "step=43 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [367.84543 189.3296 ]\n",
      "step=44 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [364.80423 180.19998]\n",
      "step=45 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [370.269   176.18233]\n",
      "step=46 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [367.61572 176.67043]\n",
      "step=47 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [365.67737 171.58052]\n",
      "step=48 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [372.04837 166.10663]\n",
      "step=49 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [365.5092  172.93164]\n",
      "step=50 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [375.76886 170.416  ]\n",
      "step=51 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [373.4582  169.16228]\n",
      "step=52 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [369.19202 137.92691]\n",
      "step=53 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [371.2429 162.3407]\n",
      "step=54 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [369.3552  162.83875]\n",
      "step=55 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [366.1019  158.22235]\n",
      "step=56 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [365.82388 160.18945]\n",
      "step=57 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [334.19928 156.38268]\n",
      "step=58 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [349.80362 164.07555]\n",
      "step=59 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [345.6116  165.06516]\n",
      "step=60 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [346.05057 214.11652]\n",
      "step=61 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [337.75983 165.84778]\n",
      "step=62 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [339.69565 180.41565]\n",
      "step=63 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [330.47556 201.19475]\n",
      "step=64 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [327.46866 203.12828]\n",
      "step=65 reward=np.float64(0.0) terminated=False\n",
      "numpy_action [337.118   223.59067]\n",
      "step=66 reward=np.float64(0.0062566358452350234) terminated=False\n",
      "numpy_action [347.3113  191.05794]\n",
      "step=67 reward=np.float64(0.009556175672773039) terminated=False\n",
      "numpy_action [350.86887 229.85559]\n",
      "step=68 reward=np.float64(0.009556175672773039) terminated=False\n",
      "numpy_action [370.5977  282.78735]\n",
      "step=69 reward=np.float64(0.02796939585958358) terminated=False\n",
      "numpy_action [400.0315 286.1195]\n",
      "step=70 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [406.1027 283.9393]\n",
      "step=71 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [403.6927  285.39355]\n",
      "step=72 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [411.28027 271.35947]\n",
      "step=73 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [322.1349  219.72672]\n",
      "step=74 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [376.0285  237.13799]\n",
      "step=75 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [363.23688 193.16817]\n",
      "step=76 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [360.68698 188.99483]\n",
      "step=77 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [357.07132 177.58537]\n",
      "step=78 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [363.5947 172.8503]\n",
      "step=79 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [353.2699  165.62169]\n",
      "step=80 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [348.07394 157.78893]\n",
      "step=81 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [346.65823 132.90283]\n",
      "step=82 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [344.68408 134.17993]\n",
      "step=83 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [336.87167 142.9765 ]\n",
      "step=84 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [322.27524 138.68817]\n",
      "step=85 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [330.6289  126.40565]\n",
      "step=86 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [323.9044  145.35236]\n",
      "step=87 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [324.8866  148.14188]\n",
      "step=88 reward=np.float64(0.03396042169245131) terminated=False\n",
      "numpy_action [326.89227 150.35641]\n",
      "step=89 reward=np.float64(0.03510301341837933) terminated=False\n",
      "numpy_action [320.21973 153.62929]\n",
      "step=90 reward=np.float64(0.052371511893532445) terminated=False\n",
      "numpy_action [315.76352 174.07312]\n",
      "step=91 reward=np.float64(0.09190022100888817) terminated=False\n",
      "numpy_action [309.72424 183.57101]\n",
      "step=92 reward=np.float64(0.14765898459382423) terminated=False\n",
      "numpy_action [306.18533 186.99022]\n",
      "step=93 reward=np.float64(0.18451771380057794) terminated=False\n",
      "numpy_action [302.2225  198.00266]\n",
      "step=94 reward=np.float64(0.21882840856946467) terminated=False\n",
      "numpy_action [286.2397  221.37308]\n",
      "step=95 reward=np.float64(0.2414951070284807) terminated=False\n",
      "numpy_action [281.03278 226.63203]\n",
      "step=96 reward=np.float64(0.2446386432014343) terminated=False\n",
      "numpy_action [276.8766  232.44525]\n",
      "step=97 reward=np.float64(0.24741151070328324) terminated=False\n",
      "numpy_action [268.78513 240.32796]\n",
      "step=98 reward=np.float64(0.31578564129424713) terminated=False\n",
      "numpy_action [263.2434  247.72485]\n",
      "step=99 reward=np.float64(0.44728147674481966) terminated=False\n",
      "numpy_action [277.07254 265.27924]\n",
      "step=100 reward=np.float64(0.5915878903534777) terminated=False\n",
      "numpy_action [257.5651 261.2803]\n",
      "step=101 reward=np.float64(0.6684944949889616) terminated=False\n",
      "numpy_action [262.94797 255.82234]\n",
      "step=102 reward=np.float64(0.6843090874935022) terminated=False\n",
      "numpy_action [281.38266 270.56937]\n",
      "step=103 reward=np.float64(0.6864611634677508) terminated=False\n",
      "numpy_action [346.1008  249.74373]\n",
      "step=104 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [354.23508 289.83264]\n",
      "step=105 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [364.47137 301.3139 ]\n",
      "step=106 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [355.56955 333.9253 ]\n",
      "step=107 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [355.31473 349.79745]\n",
      "step=108 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [332.54352 379.5624 ]\n",
      "step=109 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [326.32147 386.35138]\n",
      "step=110 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [318.41003 390.272  ]\n",
      "step=111 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [304.29233 395.4603 ]\n",
      "step=112 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [293.71738 395.37424]\n",
      "step=113 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [284.32968 391.1774 ]\n",
      "step=114 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [ 83.565575 279.4013  ]\n",
      "step=115 reward=np.float64(0.6864262754802226) terminated=False\n",
      "numpy_action [200.4398  336.78064]\n",
      "step=116 reward=np.float64(0.6834902853384429) terminated=False\n",
      "numpy_action [146.08315 300.6485 ]\n",
      "step=117 reward=np.float64(0.6675574586027294) terminated=False\n",
      "numpy_action [127.64342 276.08194]\n",
      "step=118 reward=np.float64(0.6634750242914824) terminated=False\n",
      "numpy_action [122.6227  226.75049]\n",
      "step=119 reward=np.float64(0.6634750242914824) terminated=False\n",
      "numpy_action [139.37595 244.8852 ]\n",
      "step=120 reward=np.float64(0.6634750242914824) terminated=False\n",
      "numpy_action [170.80577 206.5281 ]\n",
      "step=121 reward=np.float64(0.6634750242914824) terminated=False\n",
      "numpy_action [174.88588 203.5769 ]\n",
      "step=122 reward=np.float64(0.6634750242914824) terminated=False\n",
      "numpy_action [187.29166 206.43655]\n",
      "step=123 reward=np.float64(0.6634750242914824) terminated=False\n",
      "numpy_action [228.78781 200.77979]\n",
      "step=124 reward=np.float64(0.6634750242914824) terminated=False\n",
      "numpy_action [280.3896 194.7146]\n",
      "step=125 reward=np.float64(0.6634750242914824) terminated=False\n",
      "numpy_action [296.23743 203.96146]\n",
      "step=126 reward=np.float64(0.6231945770555359) terminated=False\n",
      "numpy_action [311.42926 216.84782]\n",
      "step=127 reward=np.float64(0.5441835950618089) terminated=False\n",
      "numpy_action [334.77286 231.4941 ]\n",
      "step=128 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [341.49484 244.87791]\n",
      "step=129 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [354.00958 265.8259 ]\n",
      "step=130 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [360.071   278.36603]\n",
      "step=131 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [363.63995 281.13452]\n",
      "step=132 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [368.46973 296.66928]\n",
      "step=133 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [367.24017 312.92627]\n",
      "step=134 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [368.81818 325.53812]\n",
      "step=135 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [355.86353 331.1728 ]\n",
      "step=136 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [347.47714 338.1996 ]\n",
      "step=137 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [363.0521  337.20227]\n",
      "step=138 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [342.83298 344.79434]\n",
      "step=139 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [337.48615 343.43427]\n",
      "step=140 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [336.28455 343.02988]\n",
      "step=141 reward=np.float64(0.5098228570389949) terminated=False\n",
      "numpy_action [329.39816 339.8661 ]\n",
      "step=142 reward=np.float64(0.5181996728821926) terminated=False\n",
      "numpy_action [326.3591  343.83072]\n",
      "step=143 reward=np.float64(0.5115354662262465) terminated=False\n",
      "numpy_action [325.15402 337.76785]\n",
      "step=144 reward=np.float64(0.5057126952299135) terminated=False\n",
      "numpy_action [322.2403  342.49008]\n",
      "step=145 reward=np.float64(0.4986177096154898) terminated=False\n",
      "numpy_action [321.5569  336.19702]\n",
      "step=146 reward=np.float64(0.49438477043463264) terminated=False\n",
      "numpy_action [337.2683 321.8429]\n",
      "step=147 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [332.8168 325.3392]\n",
      "step=148 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [320.91852 298.24622]\n",
      "step=149 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [329.93536 322.16882]\n",
      "step=150 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [338.97235 293.7139 ]\n",
      "step=151 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [340.5621  289.34503]\n",
      "step=152 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [351.60437 262.71826]\n",
      "step=153 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [321.15448 228.02617]\n",
      "step=154 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [339.04755 241.13159]\n",
      "step=155 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [311.74103 217.60896]\n",
      "step=156 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [310.84067 214.76617]\n",
      "step=157 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [298.35187 209.21912]\n",
      "step=158 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [294.73218 220.04256]\n",
      "step=159 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [290.273   215.75142]\n",
      "step=160 reward=np.float64(0.4942023827416968) terminated=False\n",
      "numpy_action [271.40698 213.05339]\n",
      "step=161 reward=np.float64(0.49078202357875184) terminated=False\n",
      "numpy_action [278.26266 222.96454]\n",
      "step=162 reward=np.float64(0.48918083997930284) terminated=False\n",
      "numpy_action [259.38443 215.2798 ]\n",
      "step=163 reward=np.float64(0.48882690645646043) terminated=False\n",
      "numpy_action [270.08136 223.81512]\n",
      "step=164 reward=np.float64(0.48882690645646043) terminated=False\n",
      "numpy_action [264.56183 226.2869 ]\n",
      "step=165 reward=np.float64(0.48882690645646043) terminated=False\n",
      "numpy_action [262.08307 230.35307]\n",
      "step=166 reward=np.float64(0.48882690645646043) terminated=False\n",
      "numpy_action [265.11932 232.47095]\n",
      "step=167 reward=np.float64(0.48882690645646043) terminated=False\n",
      "numpy_action [246.01154 231.97264]\n",
      "step=168 reward=np.float64(0.48846894632411925) terminated=False\n",
      "numpy_action [259.26746 237.97163]\n",
      "step=169 reward=np.float64(0.48846894632411925) terminated=False\n",
      "numpy_action [248.04063 226.59888]\n",
      "step=170 reward=np.float64(0.48846894632411925) terminated=False\n",
      "numpy_action [259.22922 234.4744 ]\n",
      "step=171 reward=np.float64(0.48846894632411925) terminated=False\n",
      "numpy_action [296.7087  217.28455]\n",
      "step=172 reward=np.float64(0.48562615224478317) terminated=False\n",
      "numpy_action [312.75626 225.69905]\n",
      "step=173 reward=np.float64(0.46056037827705576) terminated=False\n",
      "numpy_action [345.2054  234.97388]\n",
      "step=174 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [359.8779  248.09753]\n",
      "step=175 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [365.67056 252.25336]\n",
      "step=176 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [371.52707 278.3557 ]\n",
      "step=177 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [376.78027 289.11642]\n",
      "step=178 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [380.24973 299.30396]\n",
      "step=179 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [378.99033 306.46667]\n",
      "step=180 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [391.04523 328.9803 ]\n",
      "step=181 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [382.72363 320.01996]\n",
      "step=182 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [374.90085 324.4904 ]\n",
      "step=183 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [386.15997 336.17053]\n",
      "step=184 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [381.45386 337.28137]\n",
      "step=185 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [361.9486 347.5321]\n",
      "step=186 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [371.69186 349.18704]\n",
      "step=187 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [356.35455 351.954  ]\n",
      "step=188 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [354.0666  347.36288]\n",
      "step=189 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [347.48727 345.90536]\n",
      "step=190 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [347.23502 341.37613]\n",
      "step=191 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [361.07175 346.7288 ]\n",
      "step=192 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [344.4898  339.88757]\n",
      "step=193 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [344.20978 343.05347]\n",
      "step=194 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [344.52277 325.7194 ]\n",
      "step=195 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [341.923   337.07178]\n",
      "step=196 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [339.86902 333.7662 ]\n",
      "step=197 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [340.00705 330.84247]\n",
      "step=198 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [328.5033  331.38364]\n",
      "step=199 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [329.4663 336.1437]\n",
      "step=200 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [324.99048 332.7004 ]\n",
      "step=201 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [323.45206 333.3073 ]\n",
      "step=202 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [328.11768 325.96375]\n",
      "step=203 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [324.38797 325.48474]\n",
      "step=204 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [322.05167 330.79648]\n",
      "step=205 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [320.48895 325.6364 ]\n",
      "step=206 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [321.38647 328.7488 ]\n",
      "step=207 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [319.30615 324.33743]\n",
      "step=208 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [322.72025 323.75143]\n",
      "step=209 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [321.03482 324.3094 ]\n",
      "step=210 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [325.3319  320.67596]\n",
      "step=211 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [321.60175 317.63483]\n",
      "step=212 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [337.7845  311.12607]\n",
      "step=213 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [336.4994  303.78506]\n",
      "step=214 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [344.3201  301.82172]\n",
      "step=215 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [350.10135 291.5836 ]\n",
      "step=216 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [351.29218 293.4955 ]\n",
      "step=217 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [357.23373 260.51056]\n",
      "step=218 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [348.49698 256.95044]\n",
      "step=219 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [352.52866 248.77724]\n",
      "step=220 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [349.42123 243.39186]\n",
      "step=221 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [342.91284 241.67667]\n",
      "step=222 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [341.05655 245.60403]\n",
      "step=223 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [340.05783 244.86807]\n",
      "step=224 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [344.72458 250.7792 ]\n",
      "step=225 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [341.59756 252.70256]\n",
      "step=226 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [327.26917 262.75885]\n",
      "step=227 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [340.37048 271.16443]\n",
      "step=228 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [338.71423 267.47757]\n",
      "step=229 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [342.17624 263.90332]\n",
      "step=230 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [329.95984 275.03067]\n",
      "step=231 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [341.14197 264.99432]\n",
      "step=232 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [337.16418 274.9536 ]\n",
      "step=233 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [340.7139  280.58005]\n",
      "step=234 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [345.7976 267.841 ]\n",
      "step=235 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [341.5857  281.30914]\n",
      "step=236 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [340.09564 284.79257]\n",
      "step=237 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [344.9271 282.6798]\n",
      "step=238 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [336.7798 289.1972]\n",
      "step=239 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [338.56766 295.31314]\n",
      "step=240 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [338.02325 302.03165]\n",
      "step=241 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [335.8954  305.06183]\n",
      "step=242 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [336.85236 316.4969 ]\n",
      "step=243 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [335.87946 314.02768]\n",
      "step=244 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [339.608  311.0426]\n",
      "step=245 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [335.89816 325.2914 ]\n",
      "step=246 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [339.62305 327.6425 ]\n",
      "step=247 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [331.84546 331.21817]\n",
      "step=248 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [333.63196 334.5672 ]\n",
      "step=249 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [331.37012 342.72705]\n",
      "step=250 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [327.58554 342.81143]\n",
      "step=251 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [326.10965 343.2272 ]\n",
      "step=252 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [329.95166 342.2908 ]\n",
      "step=253 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [329.48422 349.60468]\n",
      "step=254 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [321.29065 340.06403]\n",
      "step=255 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [322.73578 338.99292]\n",
      "step=256 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [326.8805  333.63956]\n",
      "step=257 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [323.88916 334.81586]\n",
      "step=258 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [319.4259  333.55722]\n",
      "step=259 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [319.21283 330.97623]\n",
      "step=260 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [317.58603 337.1318 ]\n",
      "step=261 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [319.73074 340.4056 ]\n",
      "step=262 reward=np.float64(0.4380025371302102) terminated=False\n",
      "numpy_action [298.7576  358.00504]\n",
      "step=263 reward=np.float64(0.43718807505951346) terminated=False\n",
      "numpy_action [298.38608 370.79425]\n",
      "step=264 reward=np.float64(0.42038117302682837) terminated=False\n",
      "numpy_action [295.68527 377.6269 ]\n",
      "step=265 reward=np.float64(0.40406880599602235) terminated=False\n",
      "numpy_action [278.12335 389.45236]\n",
      "step=266 reward=np.float64(0.38500717647869426) terminated=False\n",
      "numpy_action [274.0926  391.61737]\n",
      "step=267 reward=np.float64(0.37013316401092794) terminated=False\n",
      "numpy_action [262.0099 374.7954]\n",
      "step=268 reward=np.float64(0.3634518872026763) terminated=False\n",
      "numpy_action [257.3155 393.4925]\n",
      "step=269 reward=np.float64(0.3653967169568587) terminated=False\n",
      "numpy_action [252.25146 373.39114]\n",
      "step=270 reward=np.float64(0.37106286364010344) terminated=False\n",
      "numpy_action [242.30043 371.05612]\n",
      "step=271 reward=np.float64(0.3819415344161959) terminated=False\n",
      "numpy_action [244.50911 378.62415]\n",
      "step=272 reward=np.float64(0.3915002674897168) terminated=False\n",
      "numpy_action [241.2722 365.0348]\n",
      "step=273 reward=np.float64(0.4012460533622436) terminated=False\n",
      "numpy_action [234.64348 365.2968 ]\n",
      "step=274 reward=np.float64(0.41636133905126693) terminated=False\n",
      "numpy_action [233.8024 359.4573]\n",
      "step=275 reward=np.float64(0.4344528112755479) terminated=False\n",
      "numpy_action [241.68585 376.34616]\n",
      "step=276 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [241.94182 384.14734]\n",
      "step=277 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [257.04544 388.66043]\n",
      "step=278 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [269.2851  380.93073]\n",
      "step=279 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [270.1443  380.30472]\n",
      "step=280 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [277.1968  376.12582]\n",
      "step=281 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [280.50955 368.30597]\n",
      "step=282 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [285.6863  369.88367]\n",
      "step=283 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [287.30426 362.51077]\n",
      "step=284 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [282.14362 373.0673 ]\n",
      "step=285 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [290.56464 360.30444]\n",
      "step=286 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [292.2714 358.0742]\n",
      "step=287 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [290.79517 358.5802 ]\n",
      "step=288 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [297.68192 348.6157 ]\n",
      "step=289 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [299.18253 346.93408]\n",
      "step=290 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [293.2788  351.14374]\n",
      "step=291 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [300.20068 349.99133]\n",
      "step=292 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [306.59323 353.20175]\n",
      "step=293 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [292.8495  357.48633]\n",
      "step=294 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [293.03052 358.92618]\n",
      "step=295 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [287.2977 362.3979]\n",
      "step=296 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [284.36172 366.40945]\n",
      "step=297 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [239.82343 364.82806]\n",
      "step=298 reward=np.float64(0.43676195551910324) terminated=False\n",
      "numpy_action [264.53363 358.2335 ]\n",
      "step=299 reward=np.float64(0.43676195551910324) terminated=False\n",
      "Failure!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (680, 680) to (688, 688) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of the evaluation is available in 'outputs/train/test_bet_30k_off1000/rollout.mp4'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import gym_pusht  # noqa: F401\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# pretrained_policy_path = Path(\"outputs/train/test_bet\")\n",
    "\n",
    "# trained_policy = DiffusionPolicy.from_pretrained(pretrained_policy_path)\n",
    "\n",
    "# Initialize evaluation environment to render two observation types:\n",
    "# an image of the scene and state/position of the agent. The environment\n",
    "# also automatically stops running after 300 interactions/steps.\n",
    "env = gym.make(\n",
    "    \"gym_pusht/PushT-v0\",\n",
    "    obs_type=\"pixels_agent_pos\",\n",
    "    max_episode_steps=300,\n",
    ")\n",
    "\n",
    "# We can verify that the shapes of the features expected by the policy match the ones from the observations\n",
    "# produced by the environment\n",
    "print(policy.config.input_features)\n",
    "print(env.observation_space)\n",
    "\n",
    "# Similarly, we can check that the actions produced by the policy will match the actions expected by the\n",
    "# environment\n",
    "print(policy.config.output_features)\n",
    "print(env.action_space)\n",
    "\n",
    "# Reset the policy and environments to prepare for rollout\n",
    "policy.reset()\n",
    "numpy_observation, info = env.reset(seed=42)\n",
    "\n",
    "# Prepare to collect every rewards and all the frames of the episode,\n",
    "# from initial state to final state.\n",
    "rewards = []\n",
    "frames = []\n",
    "\n",
    "# Render frame of the initial state\n",
    "frames.append(env.render())\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    # Prepare observation for the policy running in Pytorch\n",
    "    state = torch.from_numpy(numpy_observation[\"agent_pos\"])\n",
    "    image = torch.from_numpy(numpy_observation[\"pixels\"])\n",
    "\n",
    "    # Convert to float32 with image from channel first in [0,255]\n",
    "    # to channel last in [0,1]\n",
    "    state = state.to(torch.float32)\n",
    "    image = image.to(torch.float32) / 255\n",
    "    image = image.permute(2, 0, 1)\n",
    "\n",
    "    # Send data tensors from CPU to GPU\n",
    "    state = state.to(device, non_blocking=True)\n",
    "    image = image.to(device, non_blocking=True)\n",
    "\n",
    "    # Add extra (empty) batch dimension, required to forward the policy\n",
    "    state = state.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Create the policy input dictionary\n",
    "    observation = {\n",
    "        \"observation.state\": state,\n",
    "        \"observation.image\": image,\n",
    "    }\n",
    "\n",
    "    # Predict the next action with respect to the current observation\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(observation)\n",
    "\n",
    "    # Prepare the action for the environment\n",
    "    numpy_action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "    print(f\"numpy_action {numpy_action}\")\n",
    "    # Step through the environment and receive a new observation\n",
    "    numpy_observation, reward, terminated, truncated, info = env.step(numpy_action)\n",
    "    print(f\"{step=} {reward=} {terminated=}\")\n",
    "\n",
    "    # Keep track of all the rewards and frames\n",
    "    rewards.append(reward)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    # The rollout is considered done when the success state is reached (i.e. terminated is True),\n",
    "    # or the maximum number of iterations is reached (i.e. truncated is True)\n",
    "    done = terminated | truncated | done\n",
    "    step += 1\n",
    "\n",
    "if terminated:\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"Failure!\")\n",
    "\n",
    "# Get the speed of environment (i.e. its number of frames per second).\n",
    "fps = env.metadata[\"render_fps\"]\n",
    "\n",
    "# Encode all frames into a mp4 video.\n",
    "video_path = output_directory / \"rollout.mp4\"\n",
    "imageio.mimsave(str(video_path), numpy.stack(frames), fps=fps)\n",
    "\n",
    "print(f\"Video of the evaluation is available in '{video_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251e4a2-e8ff-4cd6-84d6-a14c69352db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
